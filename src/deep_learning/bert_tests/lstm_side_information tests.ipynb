{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense \n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv(\"../../../data/side_information.csv\",encoding='unicode_escape')\n",
    "checkpoint_path = \"C:/Users/jack-/Documents/University/Project/src/deep_learning/bert_tests/checkpoints\"\n",
    "feature_names = ['Sentence',\n",
    "                 'Length in Words', 'Length in Characters', 'F-score', 'I-score',\n",
    "                 'Lexical Density','FK Reading Ease', 'FOG Scale', 'SMOG Index', 'ARI',\n",
    "                 'CL Index', 'LW Formula', 'DC Score', 'Readability Consensus',\n",
    "                 'Spache Formula']\n",
    "\n",
    "samples = data[feature_names]\n",
    "labels = data[\"Formality\"]\n",
    "train_samples, test_samples, train_labels,test_labels = train_test_split(samples, labels, test_size=0.2,random_state=5)\n",
    "\n",
    "bert_train_samples = np.array(train_samples[\"Sentence\"])\n",
    "bert_test_samples = np.array(test_samples[\"Sentence\"])\n",
    "side_train_samples = np.array(train_samples[feature_names[1:]])\n",
    "side_test_samples = np.array(test_samples[feature_names[1:]])\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "\n",
    "# Attention layer\n",
    "class peel_the_layer(tf.keras.layers.Layer): \n",
    "\n",
    "    def __init__(self,units=1):    \n",
    "        ##Nothing special to be done here\n",
    "        super(peel_the_layer, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        ##Define the shape of the weights and bias in this layer\n",
    "        ##This is a 1 unit layer. \n",
    "        units=1\n",
    "        ##last index of the input_shape is the number of dimensions of the prev\n",
    "        ##RNN layer. last but 1 index is the num of timesteps\n",
    "        self.w=self.add_weight(name=\"att_weights\", shape=(input_shape[-1], units), initializer=\"normal\") #name property is useful for avoiding RuntimeError: Unable to create link.\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[-2], units), initializer=\"zeros\")\n",
    "        super(peel_the_layer,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        ##x is the input tensor..each word that needs to be attended to\n",
    "        ##Below is the main processing done during training\n",
    "        ##K is the Keras Backend import\n",
    "        e = K.tanh(K.dot(x,self.w)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "\n",
    "        ##return the ouputs. 'a' is the set of attention weights\n",
    "        ##the second variable is the 'attention adjusted o/p state' or context\n",
    "        return a, K.sum(output, axis=1)\n",
    "\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "\n",
    "\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/4'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliser = tf.keras.layers.Normalization()\n",
    "normaliser.adapt(side_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NORMALISED SIDE INFORMATION MODEL --\n",
    "\n",
    "# Bert model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "normalised_bert = tf.keras.Model(text_input, lstm)\n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(14))\n",
    "normalised = normaliser(side_input)\n",
    "reshaped = tf.reshape(normalised,[-1, 1, 14])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "normalised_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Combine models and predict\n",
    "combined = tf.keras.layers.concatenate([normalised_bert.output, normalised_side.output],axis=1)\n",
    "a, context = peel_the_layer()(combined)\n",
    "dense = tf.keras.layers.Dense(1)(context)\n",
    "normalised_model = tf.keras.Model(inputs=[normalised_bert.input, normalised_side.input], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PRETRAINED SIDE MODEL -- \n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(14),name=\"Side Information\")\n",
    "reshaped = tf.reshape(side_input,[-1, 1, 14])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "pretrained_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Pre-train side model\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3,min_delta=0.01)\n",
    "pretrained_side.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "pretrained_side.fit(x=side_train_samples,y=train_labels,batch_size=5,epochs=100,verbose=0,callbacks=[callback])\n",
    "\n",
    "\n",
    "# Bert using pretrained side model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "side_input = tf.keras.layers.Input(shape=(14),)\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "a, context = peel_the_layer()(lstm)\n",
    "\n",
    "trained_side_input = pretrained_side(side_input)\n",
    "trained_side_input = tf.reshape(trained_side_input,[-1, 512])\n",
    "\n",
    "concat = tf.keras.layers.concatenate([context, trained_side_input]) \n",
    "\n",
    "\n",
    "dense = tf.keras.layers.Dense(1)(concat)\n",
    "\n",
    "pretrained_model = tf.keras.Model([text_input,side_input],dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- AVERAGE LAYER INSTEAD OF CONCATENATION --\n",
    "\n",
    "# Bert model\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "encoder_inputs = preprocessing_layer(text_input)\n",
    "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "net = outputs['pooled_output']\n",
    "reshaped = tf.reshape(net,[-1, 768, 1])\n",
    "lstm = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "average_bert = tf.keras.Model(text_input, lstm)\n",
    "\n",
    "# Side information model\n",
    "side_input = tf.keras.layers.Input(shape=(14))\n",
    "reshaped = tf.reshape(side_input,[-1, 1, 14])\n",
    "lstm_1 = tf.keras.layers.LSTM(512,return_sequences=True)(reshaped)\n",
    "lstm_2 = tf.keras.layers.LSTM(512,return_sequences=True)(lstm_1)\n",
    "average_side = tf.keras.Model(side_input, lstm_2)\n",
    "\n",
    "# Combine models and predict\n",
    "combined = tf.keras.layers.average([average_bert.output, average_side.output])\n",
    "a, context = peel_the_layer()(combined)\n",
    "dense = tf.keras.layers.Dense(1)(context)\n",
    "average_model = tf.keras.Model(inputs=[average_bert.input, average_side.input], outputs=dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- NORMALISED TRAINING AND TESTING --\n",
    "normalised_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1,min_delta=0.01)\n",
    "\n",
    "normalised_model.save_weights('normalised.h5')\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_model.load_weights(\"normalised.h5\")\n",
    "normalised_model = tf.keras.models.clone_model(normalised_model)\n",
    "normalised_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "normalised_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = normalised_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "normalised_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "normalised_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE NORMALISED RESULTS --\n",
    "normalised_results = normalised_results.T\n",
    "normalised_results.to_csv(\"/lstm_side_information_tests/normalised.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --PRETRAINED TRAINING AND TESTING --\n",
    "pretrained_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1,min_delta=0.01)\n",
    "\n",
    "pretrained_model.save_weights('pretrained.h5')\n",
    "pretrained_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "pretrained_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = pretrained_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "pretrained_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "pretrained_model.load_weights('pretrained.h5')\n",
    "pretrained_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "pretrained_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = pretrained_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "pretrained_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "pretrained_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE PRETRAINED RESULTS --\n",
    "pretrained_results = pretrained_results.T\n",
    "pretrained_results.to_csv(\"/lstm_side_information_tests/pretrained.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --AVERAGE LAYER TRAINING AND TESTING --\n",
    "average_results = pd.DataFrame()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1,min_delta=0.01)\n",
    "\n",
    "\n",
    "average_model.save_weights('average.h5')\n",
    "average_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "average_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=8,verbose=2,callbacks=[callback])\n",
    "scores = average_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "average_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 8 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "average_model.load_weights('average.h5')\n",
    "average_model.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])\n",
    "average_model.fit(x=[bert_train_samples,side_train_samples],y=train_labels,batch_size=32,epochs=20,verbose=2,callbacks=[callback])\n",
    "scores = average_model.evaluate(x=[bert_test_samples,side_test_samples],y=test_labels)\n",
    "average_results[\"E8 B32\"] = scores\n",
    "print(\"Completed 20 Epochs\")\n",
    "print(scores)\n",
    "\n",
    "average_results.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SAVE AVERAGE RESULTS --\n",
    "average_results = average_results.T\n",
    "average_results.to_csv(\"/lstm_side_information_tests/average.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2292f7dadeec0b36dafabb7d1d8dd7b9b8b8f0665c515bec64e67bf9650aaf0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
