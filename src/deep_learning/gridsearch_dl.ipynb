{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv(\"../../data/mturk_experiment_2.csv\",encoding='unicode_escape')\n",
    "labels = data[\"Formality\"]\n",
    "samples = data[\"Sentence\"]\n",
    "\n",
    "train_samples, test_samples, train_labels,test_labels = train_test_split(samples, labels, test_size=0.2)\n",
    "\n",
    "train_samples = np.array(train_samples)\n",
    "test_samples = np.array(test_samples)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding setup\n",
    "vectoriser = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectoriser.adapt(text_ds)\n",
    "\n",
    "voc = vectoriser.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "glove_path = \"glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(glove_path,encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs,\"f\",sep= \" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens,embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "embedding_layer = Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN setup\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(1)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(1)(x)\n",
    "x = layers.Conv1D(128, 1, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(units=1)(x)\n",
    "cnn = keras.Model(int_sequences_input, preds)\n",
    "cnn.compile(optimizer=Adam(learning_rate=0.0001),loss='mean_squared_error',metrics=[tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM setup\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.LSTM(4,input_shape=(1,5))(embedded_sequences)\n",
    "out = layers.Dense(1,activation='relu')(x)\n",
    "\n",
    "lstm = keras.Model(int_sequences_input,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN setup\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.GRU(256,return_sequences=True)(embedded_sequences)\n",
    "x = layers.SimpleRNN(128)(x)\n",
    "out = layers.Dense(1,activation='relu')(x)\n",
    "\n",
    "rnn = keras.Model(int_sequences_input,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data setup\n",
    "x_train = vectoriser(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectoriser(np.array([[s] for s in test_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(test_labels)\n",
    "\n",
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 5s 8ms/step - loss: 2.0051 - mean_absolute_error: 1.0921 - mean_absolute_percentage_error: 28.8626\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 4s 7ms/step - loss: 1.0690 - mean_absolute_error: 0.8267 - mean_absolute_percentage_error: 22.2556\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 4s 7ms/step - loss: 1.0163 - mean_absolute_error: 0.8110 - mean_absolute_percentage_error: 21.9320\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 4s 7ms/step - loss: 0.9791 - mean_absolute_error: 0.7885 - mean_absolute_percentage_error: 21.3167\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 4s 8ms/step - loss: 0.9474 - mean_absolute_error: 0.7813 - mean_absolute_percentage_error: 21.0939\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 5s 8ms/step - loss: 0.9166 - mean_absolute_error: 0.7648 - mean_absolute_percentage_error: 20.6889\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 5s 8ms/step - loss: 0.8986 - mean_absolute_error: 0.7600 - mean_absolute_percentage_error: 20.5475\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 4s 7ms/step - loss: 0.8903 - mean_absolute_error: 0.7541 - mean_absolute_percentage_error: 20.3042\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 4s 7ms/step - loss: 0.8644 - mean_absolute_error: 0.7413 - mean_absolute_percentage_error: 19.9651\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 4s 8ms/step - loss: 0.8267 - mean_absolute_error: 0.7259 - mean_absolute_percentage_error: 19.5901\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 6s 10ms/step - loss: 0.7942 - mean_absolute_error: 0.7098 - mean_absolute_percentage_error: 19.0600\n",
      "Epoch 12/100\n",
      "143/563 [======>.......................] - ETA: 4s - loss: 0.7890 - mean_absolute_error: 0.7129 - mean_absolute_percentage_error: 19.5173"
     ]
    }
   ],
   "source": [
    "# Try CNN with batch sizes 10,32 and epochs 50,75 and 100\n",
    "cnn.fit(x=x_train,y=y_train,batch_size=10,epochs=100)\n",
    "results[\"CNN E100 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "cnn.fit(x=x_train,y=y_train,batch_size=32,epochs=100)\n",
    "results[\"CNN E100 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "cnn.fit(x=x_train,y=y_train,batch_size=10,epochs=75)\n",
    "results[\"CNN E75 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "cnn.fit(x=x_train,y=y_train,batch_size=32,epochs=75)\n",
    "results[\"CNN E75 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "cnn.fit(x=x_train,y=y_train,batch_size=10,epochs=50)\n",
    "results[\"CNN E50 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "cnn.fit(x=x_train,y=y_train,batch_size=32,epochs=50)\n",
    "results[\"CNN E50 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try LSTM with batch sizes 10,32 and epochs 50,75 and 100\n",
    "lstm.fit(x=x_train,y=y_train,batch_size=10,epochs=100)\n",
    "results[\"LSTM E100 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "lstm.fit(x=x_train,y=y_train,batch_size=32,epochs=100)\n",
    "results[\"LSTM E100 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "lstm.fit(x=x_train,y=y_train,batch_size=10,epochs=75)\n",
    "results[\"LSTM E75 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "lstm.fit(x=x_train,y=y_train,batch_size=32,epochs=75)\n",
    "results[\"LSTM E75 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "lstm.fit(x=x_train,y=y_train,batch_size=10,epochs=50)\n",
    "results[\"LSTM E50 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "lstm.fit(x=x_train,y=y_train,batch_size=32,epochs=50)\n",
    "results[\"LSTM E50 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try RNN with batch sizes 10,32 and epochs 50,75 and 100\n",
    "rnn.fit(x=x_train,y=y_train,batch_size=10,epochs=100)\n",
    "results[\"RNN E100 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "rnn.fit(x=x_train,y=y_train,batch_size=32,epochs=100)\n",
    "results[\"RNN E100 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "rnn.fit(x=x_train,y=y_train,batch_size=10,epochs=75)\n",
    "results[\"RNN E75 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "rnn.fit(x=x_train,y=y_train,batch_size=32,epochs=75)\n",
    "results[\"RNN E75 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "rnn.fit(x=x_train,y=y_train,batch_size=10,epochs=50)\n",
    "results[\"RNN E50 B10\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)\n",
    "\n",
    "rnn.fit(x=x_train,y=y_train,batch_size=32,epochs=50)\n",
    "results[\"RNN E50 B32\"] = cnn.evaluate(x=x_val,y=y_val,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2292f7dadeec0b36dafabb7d1d8dd7b9b8b8f0665c515bec64e67bf9650aaf0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
