{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import precision_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from official.nlp import optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "data = pd.read_csv(\"../../data/sensitivity_data/sensitivity_dataset.csv\")\n",
    "data = data[[\"Document\",\"Sensitivity\"]]\n",
    "\n",
    "# Train / Test split\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(data['Document'],data['Sensitivity'],test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=500000)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vector(glove_vec):\n",
    "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "      w_line = line.split()\n",
    "      curr_word = w_line[0]\n",
    "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "      \n",
    "  return word_to_vec_map\n",
    "\n",
    "word_to_vec_map = read_glove_vector(\"..\\deep_learning\\glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 15000\n",
    "\n",
    "vocab_len = len(words_to_index)\n",
    "embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "for word, index in words_to_index.items():\n",
    "  embedding_vector = word_to_vec_map.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    emb_matrix[index, :] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_indices = tf.keras.Input(shape=(maxLen,))\n",
    "embeddings = embedding_layer(x_indices)\n",
    "x = tf.keras.layers.LSTM(128, return_sequences=True)(embeddings)\n",
    "x = tf.keras.layers.Dropout(0.6)(x)\n",
    "x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "x = tf.keras.layers.Dropout(0.6)(x)\n",
    "x = tf.keras.layers.LSTM(128)(x)\n",
    "x = tf.keras.layers.Dense(1, activation=None)(x)\n",
    "model = tf.keras.Model(inputs=x_indices,outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_sampler = RandomOverSampler(random_state=5)\n",
    "balanced_x, balanced_y = over_sampler.fit_resample(train_x, train_y)\n",
    "\n",
    "train_x_indices = tokenizer.texts_to_sequences(balanced_x)\n",
    "train_x_indices = pad_sequences(train_x_indices, maxlen=maxLen, padding='post')\n",
    "train_x_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "epochs = 5\n",
    "steps_per_epoch = 3300000000 / (256*512)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer,loss=loss,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x_indices,balanced_y,batch_size=32, epochs=epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_indices = tokenizer.texts_to_sequences(test_x)\n",
    "test_x_indices = pad_sequences(test_x_indices,maxlen=maxLen,padding='post')\n",
    "\n",
    "preds_non_binary = model.predict(test_x_indices)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
